{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec692eb",
   "metadata": {},
   "source": [
    "# Generar N frames sintéticos + \"labels\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf0319",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "937952da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mmcv\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from NDStandardScaler import NDStandardScaler\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d2bf5",
   "metadata": {},
   "source": [
    "### Paso 1\n",
    "Procesar los N (14671) frames sintéticos de Blender con un modelo de pose detection ya entrenado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc43d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth\n",
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192-81c58e40_20220909.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14660/14660, 2.0 task/s, elapsed: 7379s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crv\\miniconda3\\envs\\openmmlab\\lib\\site-packages\\json_tricks\\encoders.py:394: UserWarning: json-tricks: numpy scalar serialization is experimental and may work differently in future versions\n",
      "  warnings.warn('json-tricks: numpy scalar serialization is experimental and may work differently in future versions')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions have been saved at outputs/Blender//results_1630-16300.json\n"
     ]
    }
   ],
   "source": [
    "%run demo/topdown_demo_with_mmdet \\\n",
    "    demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py \\\n",
    "    https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \\\n",
    "    configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192.py \\\n",
    "    https://download.openmmlab.com/mmpose/v1/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w32_8xb64-210e_coco-256x192-81c58e40_20220909.pth \\\n",
    "    --input inputs/1630-16300.mp4 --save-predictions \\\n",
    "    --output-root outputs/Blender/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e821f39",
   "metadata": {},
   "source": [
    "### Paso 2\n",
    "Entrenar un modelo de aprendizaje supervisado para predecir la posición (x, y) del \"heap\" del jugador proyectado en la pista a partir de los keypoints de la jugador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "78a3241d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14660\n",
      "14660\n"
     ]
    }
   ],
   "source": [
    "# Obtener los keypoints del fichero json\n",
    "data = json.load(open('outputs/Blender/results_1630-16300.json'))\n",
    "# print(data['instance_info'][0]['instances'][0]['keypoints'])\n",
    "keypoints = []\n",
    "for i in range(0, len(data['instance_info'])):\n",
    "    if len(data['instance_info'][i]['instances']) > 0:\n",
    "        keypoints.append(data['instance_info'][i]['instances'][0]['keypoints'])\n",
    "    else:\n",
    "        keypoints.append([])\n",
    "print(len(keypoints))\n",
    "# Obtener las posiciones del 'heap' del jugador\n",
    "ground_truth = list()\n",
    "\n",
    "with open('outputs/Blender/labels-1630-16300.csv','r') as fp: # open the csv file for reading (will close when \"with\" block ends)\n",
    "    for line in fp.readlines(): # go over remaining lines\n",
    "        col = line.strip().split(\";\") # get the columns of data\n",
    "        new_item = [col[1],col[2]] # keep only 2nd and 3th items in a new list, starting count from 0, since this is how lists work in python\n",
    "        ground_truth.append(new_item) # add the current item to the item list\n",
    "ground_truth = ground_truth[0:14660]\n",
    "print(len(ground_truth))\n",
    "\n",
    "for i in range(0, len(ground_truth)):\n",
    "    keypoints[i].append(ground_truth[i])\n",
    "    \n",
    "# field names \n",
    "fields = ['k1', 'k2', 'k3', 'k4', 'k5', 'k6', 'k7', 'k8', 'k9', 'k10', 'k11', 'k12', 'k13', 'k14', 'k15', 'k17', 'k18', 'pos']\n",
    "with open('data.csv', 'w') as f:\n",
    "      \n",
    "    # using csv.writer method from CSV package\n",
    "    write = csv.writer(f)\n",
    "      \n",
    "    write.writerow(fields)\n",
    "    write.writerows(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4682376b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14660/14660, 46.8 task/s, elapsed: 313s, ETA:     0s\n"
     ]
    }
   ],
   "source": [
    "# Obtener los frames \n",
    "video = mmcv.VideoReader('outputs/Blender/1630-16300.mp4')\n",
    "video.cvt2frames('outdir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecd5e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10246, 17, 2)\n",
      "(10246, 2)\n",
      "(4392, 17, 2)\n",
      "(4392, 2)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\crv\\miniconda3\\envs\\openmmlab\\lib\\site-packages\\keras\\initializers\\initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410/410 [==============================] - 1s 998us/step - loss: 0.2093\n",
      "Epoch 2/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.1056\n",
      "Epoch 3/20\n",
      "410/410 [==============================] - 0s 997us/step - loss: 0.0939\n",
      "Epoch 4/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0877\n",
      "Epoch 5/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0836\n",
      "Epoch 6/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0797\n",
      "Epoch 7/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0775\n",
      "Epoch 8/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0758\n",
      "Epoch 9/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0744\n",
      "Epoch 10/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0724\n",
      "Epoch 11/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0714\n",
      "Epoch 12/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0703\n",
      "Epoch 13/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0685\n",
      "Epoch 14/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0668\n",
      "Epoch 15/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0666\n",
      "Epoch 16/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0657\n",
      "Epoch 17/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0650\n",
      "Epoch 18/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0637\n",
      "Epoch 19/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0629\n",
      "Epoch 20/20\n",
      "410/410 [==============================] - 0s 1ms/step - loss: 0.0619\n",
      "138/138 [==============================] - 0s 728us/step\n",
      "[[ 0.66498166 -1.1098788 ]\n",
      " [-0.64364177  0.9027856 ]\n",
      " [ 1.0376388  -1.140264  ]\n",
      " [ 0.42967358  0.8612338 ]\n",
      " [-0.21388532  0.67144936]\n",
      " [-1.5966868  -0.960984  ]\n",
      " [-0.2558125   1.3084648 ]\n",
      " [-0.81323296  1.0808663 ]\n",
      " [-1.0801656  -0.9085689 ]\n",
      " [ 0.03770591  0.11772224]]\n",
      "138/138 [==============================] - 0s 641us/step\n",
      "Accuracy: 94.78196200096633\n"
     ]
    }
   ],
   "source": [
    "#dataset = pd.read_csv('data.csv')\n",
    "# Splitting the Data into Training and Testing\n",
    "#TargetVariable = ['pos']\n",
    "#Predictors = ['k1', 'k2', 'k3', 'k4', 'k5', 'k6', 'k7', 'k8', 'k9', 'k10', 'k11', 'k12', 'k13', 'k14', 'k15', 'k17', 'k18']\n",
    "\n",
    "data = json.load(open('outputs/Blender/results_1630-16300.json'))\n",
    "# print(data['instance_info'][0]['instances'][0]['keypoints'])\n",
    "zeros = []\n",
    "X = []\n",
    "for i in range(0, len(data['instance_info'])):\n",
    "    if len(data['instance_info'][i]['instances']) > 0:\n",
    "        X.append(data['instance_info'][i]['instances'][0]['keypoints'])\n",
    "    else:\n",
    "        zeros.append(i)\n",
    "        \n",
    "zeros.sort(reverse = True)\n",
    "\n",
    "y = list()\n",
    "with open('outputs/Blender/labels-1630-16300.csv','r') as fp: # open the csv file for reading (will close when \"with\" block ends)\n",
    "    for line in fp.readlines(): # go over remaining lines\n",
    "        col = line.strip().split(\";\") # get the columns of data\n",
    "        new_item = [col[1],col[2]] # keep only 2nd and 3th items in a new list, starting count from 0, since this is how lists work in python\n",
    "        y.append(new_item) # add the current item to the item list\n",
    "y = y[0:14660]\n",
    "\n",
    "for i in zeros:\n",
    "    y.pop(i)\n",
    "\n",
    "### Sandardization of data ###\n",
    "PredictorScaler = NDStandardScaler()\n",
    "TargetVarScaler = NDStandardScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit = PredictorScaler.fit(X)\n",
    "TargetVarScalerFit = TargetVarScaler.fit(y)\n",
    "\n",
    "# Generating the standardized values of X and y\n",
    "X = PredictorScalerFit.transform(X)\n",
    "y = TargetVarScalerFit.transform(y)\n",
    " \n",
    "# Split the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# create ANN model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    " \n",
    "initializer = tf.keras.initializers.GlorotNormal()\n",
    "    \n",
    "# Defining the Input layer and FIRST hidden layer, both are same!\n",
    "model.add(Dense(units=200, input_dim=17, kernel_initializer=initializer, activation='selu'))\n",
    " \n",
    "# Defining the Second layer of the model\n",
    "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "model.add(Dense(units=200, kernel_initializer=initializer, activation='elu'))\n",
    " \n",
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(2, kernel_initializer=initializer))\n",
    " \n",
    "# Compiling the model\n",
    "model.compile(loss=\"MeanAbsoluteError\", optimizer='rmsprop')\n",
    " \n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train ,batch_size = 25, epochs = 20, verbose=1)\n",
    "Predictions=model.predict(X_test)\n",
    "print(Predictions[0:10])\n",
    "MAPE = np.mean(100 * (np.abs(y_test-model.predict(X_test))/y_test))\n",
    "print('Accuracy:', 100-MAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c453fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 554us/step\n",
      "1 Parameters: batch_size: 5 - epochs: 5 Accuracy: 52.013902826252576\n",
      "138/138 [==============================] - 0s 531us/step\n",
      "2 Parameters: batch_size: 5 - epochs: 10 Accuracy: 48.726907546403076\n",
      "138/138 [==============================] - 0s 524us/step\n",
      "3 Parameters: batch_size: 5 - epochs: 50 Accuracy: 51.808766890992686\n",
      "138/138 [==============================] - 0s 534us/step\n",
      "4 Parameters: batch_size: 5 - epochs: 100 Accuracy: 52.52089433952289\n",
      "138/138 [==============================] - 0s 524us/step\n",
      "5 Parameters: batch_size: 10 - epochs: 5 Accuracy: 51.746367491161735\n",
      "138/138 [==============================] - 0s 539us/step\n",
      "6 Parameters: batch_size: 10 - epochs: 10 Accuracy: 51.311876586288335\n",
      "138/138 [==============================] - 0s 531us/step\n",
      "7 Parameters: batch_size: 10 - epochs: 50 Accuracy: 51.41356720939623\n",
      "138/138 [==============================] - 0s 528us/step\n",
      "8 Parameters: batch_size: 10 - epochs: 100 Accuracy: 52.17183317049977\n",
      "138/138 [==============================] - 0s 542us/step\n",
      "9 Parameters: batch_size: 15 - epochs: 5 Accuracy: 49.44382612308696\n",
      "138/138 [==============================] - 0s 543us/step\n",
      "10 Parameters: batch_size: 15 - epochs: 10 Accuracy: 49.55383642147289\n",
      "138/138 [==============================] - 0s 529us/step\n",
      "11 Parameters: batch_size: 15 - epochs: 50 Accuracy: 53.20807825569728\n",
      "138/138 [==============================] - 0s 524us/step\n",
      "12 Parameters: batch_size: 15 - epochs: 100 Accuracy: 51.22455285622143\n",
      "138/138 [==============================] - 0s 539us/step\n",
      "13 Parameters: batch_size: 20 - epochs: 5 Accuracy: 47.15855330292078\n",
      "138/138 [==============================] - 0s 531us/step\n",
      "14 Parameters: batch_size: 20 - epochs: 10 Accuracy: 49.321290053853815\n",
      "138/138 [==============================] - 0s 539us/step\n",
      "15 Parameters: batch_size: 20 - epochs: 50 Accuracy: 51.24009013683507\n",
      "138/138 [==============================] - 0s 540us/step\n",
      "16 Parameters: batch_size: 20 - epochs: 100 Accuracy: 50.0924567940036\n"
     ]
    }
   ],
   "source": [
    "def FunctionFindBestParams(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    batch_size_list=[5, 10, 15, 20]\n",
    "    epoch_list  =   [5, 10, 50, 100]\n",
    "    \n",
    "    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])\n",
    "    \n",
    "    # initializing the trials\n",
    "    TrialNumber=0\n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            TrialNumber+=1\n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            \n",
    "            model.add(tf.keras.layers.Flatten())\n",
    "            \n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=5, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "            # The output neuron is a single fully connected node \n",
    "            # Since we will be predicting a single number\n",
    "            model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "            # Compiling the model\n",
    "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "            # Fitting the ANN to the Training set\n",
    "            model.fit(X_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=0)\n",
    " \n",
    "            MAPE = np.mean(100 * (np.abs(y_test-model.predict(X_test))/y_test))\n",
    "            \n",
    "            # printing the results of the current iteration\n",
    "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', 100-MAPE)\n",
    "            \n",
    "            SearchResultsData=pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), 100-MAPE]],\n",
    "                                                                    columns=['TrialNumber', 'Parameters', 'Accuracy'] )\n",
    "    return(SearchResultsData)\n",
    " \n",
    " \n",
    "######################################################\n",
    "# Calling the function\n",
    "ResultsData=FunctionFindBestParams(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61497f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 0s 655us/step\n",
      "[[834. 577.]\n",
      " [658. 818.]\n",
      " [871. 574.]\n",
      " [788. 799.]\n",
      " [707. 780.]\n",
      " [524. 595.]\n",
      " [705. 859.]\n",
      " [606. 842.]\n",
      " [604. 604.]\n",
      " [745. 711.]]\n",
      "[[827.5528  574.988  ]\n",
      " [654.24097 816.6083 ]\n",
      " [864.3288  564.7999 ]\n",
      " [788.4533  800.6785 ]\n",
      " [705.0659  776.06757]\n",
      " [520.332   590.91095]\n",
      " [700.9319  858.5479 ]\n",
      " [627.7033  829.4436 ]\n",
      " [594.9424  599.2205 ]\n",
      " [734.5808  713.46136]]\n"
     ]
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "\n",
    "model.fit(X_train, y_train ,batch_size = 15, epochs = 10, verbose=0)\n",
    " \n",
    "# Generating Predictions on testing data\n",
    "Predictions=model.predict(X_test)\n",
    "\n",
    "# Scaling the predicted Price data back to original price scale\n",
    "Predictions=TargetVarScalerFit.inverse_transform(Predictions)\n",
    "\n",
    "# Scaling the y_test Price data back to original price scale\n",
    "y_test_orig=TargetVarScalerFit.inverse_transform(y_test)\n",
    "\n",
    "# Scaling the test data back to original scale\n",
    "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
    "\n",
    "print(y_test_orig[0:10])\n",
    "print(Predictions[0:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
